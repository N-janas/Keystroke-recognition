{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_rows = 30\n",
    "pd.options.display.max_columns = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "In this notebook, we will take a look at the **keystrokes data** provided by collected users while typing certain text provided by us.\n",
    "We wil show the data that is provided and try to apply it to machine learning algorithms to conduct a research how well some of the AI algorithms will perform with data in our dataset.\n",
    "\n",
    "## Comments\n",
    "We are aware that our results may be somewhat skewd because of the scale in which we prepared data. The scale is rather small and unscalable, because of only **15** subjects. We also skipped a few paramteres related to **Down down time** of shift keys, because they are strictly connected to other keys *(which caused weird measurements in our dataset)*.\n",
    "Despite the above we were still intrested if data provided by our software could be classified by generic AI algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>attempt</th>\n",
       "      <th>f_DD</th>\n",
       "      <th>f_H</th>\n",
       "      <th>j_DD</th>\n",
       "      <th>j_H</th>\n",
       "      <th>shift_DD</th>\n",
       "      <th>shift_H</th>\n",
       "      <th>a_DD</th>\n",
       "      <th>a_H</th>\n",
       "      <th>r_DD</th>\n",
       "      <th>r_H</th>\n",
       "      <th>@_DD</th>\n",
       "      <th>@_H</th>\n",
       "      <th>shift_DD.1</th>\n",
       "      <th>shift_H.1</th>\n",
       "      <th>s_DD</th>\n",
       "      <th>s_H</th>\n",
       "      <th>shift_DD.2</th>\n",
       "      <th>shift_H.2</th>\n",
       "      <th>e_DD</th>\n",
       "      <th>e_H</th>\n",
       "      <th>6_DD</th>\n",
       "      <th>6_H</th>\n",
       "      <th>9_DD</th>\n",
       "      <th>9_H</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>776</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "      <td>329</td>\n",
       "      <td>380</td>\n",
       "      <td>103</td>\n",
       "      <td>449</td>\n",
       "      <td>180</td>\n",
       "      <td>793</td>\n",
       "      <td>98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>653</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>286</td>\n",
       "      <td>692</td>\n",
       "      <td>118</td>\n",
       "      <td>1454</td>\n",
       "      <td>110</td>\n",
       "      <td>268</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>442</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>269</td>\n",
       "      <td>271</td>\n",
       "      <td>118</td>\n",
       "      <td>326</td>\n",
       "      <td>137</td>\n",
       "      <td>983</td>\n",
       "      <td>160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>708</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "      <td>416</td>\n",
       "      <td>398</td>\n",
       "      <td>125</td>\n",
       "      <td>1219</td>\n",
       "      <td>168</td>\n",
       "      <td>286</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>113</td>\n",
       "      <td>631</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>212</td>\n",
       "      <td>315</td>\n",
       "      <td>125</td>\n",
       "      <td>290</td>\n",
       "      <td>135</td>\n",
       "      <td>702</td>\n",
       "      <td>116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>534</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>258</td>\n",
       "      <td>335</td>\n",
       "      <td>182</td>\n",
       "      <td>1153</td>\n",
       "      <td>124</td>\n",
       "      <td>310</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>543</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>312</td>\n",
       "      <td>341</td>\n",
       "      <td>110</td>\n",
       "      <td>321</td>\n",
       "      <td>97</td>\n",
       "      <td>662</td>\n",
       "      <td>91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>412.0</td>\n",
       "      <td>688</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>348</td>\n",
       "      <td>377</td>\n",
       "      <td>132</td>\n",
       "      <td>1372</td>\n",
       "      <td>103</td>\n",
       "      <td>329</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>909</td>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>258</td>\n",
       "      <td>121</td>\n",
       "      <td>278</td>\n",
       "      <td>108</td>\n",
       "      <td>826</td>\n",
       "      <td>73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>385.0</td>\n",
       "      <td>1044</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "      <td>385</td>\n",
       "      <td>475</td>\n",
       "      <td>120</td>\n",
       "      <td>1047</td>\n",
       "      <td>116</td>\n",
       "      <td>774</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1495</td>\n",
       "      <td>14</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>248</td>\n",
       "      <td>137</td>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>138</td>\n",
       "      <td>103</td>\n",
       "      <td>151</td>\n",
       "      <td>103</td>\n",
       "      <td>290</td>\n",
       "      <td>137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>370</td>\n",
       "      <td>137</td>\n",
       "      <td>0</td>\n",
       "      <td>269</td>\n",
       "      <td>252</td>\n",
       "      <td>112</td>\n",
       "      <td>662</td>\n",
       "      <td>117</td>\n",
       "      <td>118</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1496</td>\n",
       "      <td>14</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>232</td>\n",
       "      <td>127</td>\n",
       "      <td>0</td>\n",
       "      <td>201</td>\n",
       "      <td>162</td>\n",
       "      <td>116</td>\n",
       "      <td>134</td>\n",
       "      <td>131</td>\n",
       "      <td>296</td>\n",
       "      <td>122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>306</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>246</td>\n",
       "      <td>241</td>\n",
       "      <td>104</td>\n",
       "      <td>691</td>\n",
       "      <td>117</td>\n",
       "      <td>118</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1497</td>\n",
       "      <td>14</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>220</td>\n",
       "      <td>136</td>\n",
       "      <td>0</td>\n",
       "      <td>219</td>\n",
       "      <td>180</td>\n",
       "      <td>111</td>\n",
       "      <td>155</td>\n",
       "      <td>103</td>\n",
       "      <td>320</td>\n",
       "      <td>120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>316</td>\n",
       "      <td>137</td>\n",
       "      <td>0</td>\n",
       "      <td>261</td>\n",
       "      <td>290</td>\n",
       "      <td>119</td>\n",
       "      <td>569</td>\n",
       "      <td>121</td>\n",
       "      <td>123</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1498</td>\n",
       "      <td>14</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>227</td>\n",
       "      <td>137</td>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>223</td>\n",
       "      <td>111</td>\n",
       "      <td>699</td>\n",
       "      <td>86</td>\n",
       "      <td>336</td>\n",
       "      <td>118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>327</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>254</td>\n",
       "      <td>241</td>\n",
       "      <td>103</td>\n",
       "      <td>649</td>\n",
       "      <td>113</td>\n",
       "      <td>114</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1499</td>\n",
       "      <td>14</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>227</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>211</td>\n",
       "      <td>227</td>\n",
       "      <td>120</td>\n",
       "      <td>150</td>\n",
       "      <td>112</td>\n",
       "      <td>379</td>\n",
       "      <td>120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>353</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>236</td>\n",
       "      <td>226</td>\n",
       "      <td>111</td>\n",
       "      <td>499</td>\n",
       "      <td>112</td>\n",
       "      <td>114</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      subject  attempt  f_DD  f_H  j_DD  j_H  shift_DD  shift_H  a_DD  a_H  \\\n",
       "0           0        0     0  101   776  125         0      329   380  103   \n",
       "1           0        1     0  103   442  112         0      269   271  118   \n",
       "2           0        2     0  113   631  112         0      212   315  125   \n",
       "3           0        3     0   91   543  120         0      312   341  110   \n",
       "4           0        4     0  111   909  107         0      255   258  121   \n",
       "...       ...      ...   ...  ...   ...  ...       ...      ...   ...  ...   \n",
       "1495       14       95     0   99   248  137         0      188   138  103   \n",
       "1496       14       96     0   78   232  127         0      201   162  116   \n",
       "1497       14       97     0   99   220  136         0      219   180  111   \n",
       "1498       14       98     0  103   227  137         0      228   223  111   \n",
       "1499       14       99     0  103   227  133         0      211   227  120   \n",
       "\n",
       "      r_DD  r_H  @_DD  @_H  shift_DD.1  shift_H.1  s_DD  s_H  shift_DD.2  \\\n",
       "0      449  180   793   98         0.0      226.0   653  140           0   \n",
       "1      326  137   983  160         0.0      319.0   708  125           0   \n",
       "2      290  135   702  116         0.0      232.0   534  145           0   \n",
       "3      321   97   662   91         0.0      412.0   688  149           0   \n",
       "4      278  108   826   73         0.0      385.0  1044  117           0   \n",
       "...    ...  ...   ...  ...         ...        ...   ...  ...         ...   \n",
       "1495   151  103   290  137         0.0      295.0   370  137           0   \n",
       "1496   134  131   296  122         0.0      271.0   306  111           0   \n",
       "1497   155  103   320  120         0.0      303.0   316  137           0   \n",
       "1498   699   86   336  118         0.0      297.0   327  103           0   \n",
       "1499   150  112   379  120         0.0      282.0   353  108           0   \n",
       "\n",
       "      shift_H.2  e_DD  e_H  6_DD  6_H  9_DD  9_H  \n",
       "0           286   692  118  1454  110   268  118  \n",
       "1           416   398  125  1219  168   286   94  \n",
       "2           258   335  182  1153  124   310   70  \n",
       "3           348   377  132  1372  103   329   86  \n",
       "4           385   475  120  1047  116   774   94  \n",
       "...         ...   ...  ...   ...  ...   ...  ...  \n",
       "1495        269   252  112   662  117   118   95  \n",
       "1496        246   241  104   691  117   118   95  \n",
       "1497        261   290  119   569  121   123   86  \n",
       "1498        254   241  103   649  113   114   78  \n",
       "1499        236   226  111   499  112   114   82  \n",
       "\n",
       "[1500 rows x 26 columns]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the data\n",
    "df = pd.read_csv('results.csv', na_values='NaN', index_col=False)\n",
    "# Peak at the data\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From raw data to DataFrame\n",
    "\n",
    "Raw data provided by the software looks like this:\n",
    "![Raw data](img/raw_example.png)\n",
    "Where format is:\n",
    "    1. key;Hold_time;DownDown_time\n",
    "    2. ; - data separator\n",
    "    3. : - attempt separator\n",
    "\n",
    "## Comments\n",
    "How many subjects that many capital keys, so we are aware that our users use: *only one shift*, *both of them* or *using caps lock* but we are processing raw input with the intent to standarasie every capital type as **shift** attribute. It makes further data handling easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">attempt</th>\n",
       "      <th colspan=\"2\" halign=\"left\">f_DD</th>\n",
       "      <th colspan=\"2\" halign=\"left\">f_H</th>\n",
       "      <th colspan=\"2\" halign=\"left\">j_DD</th>\n",
       "      <th colspan=\"2\" halign=\"left\">j_H</th>\n",
       "      <th colspan=\"2\" halign=\"left\">shift_DD</th>\n",
       "      <th colspan=\"2\" halign=\"left\">shift_H</th>\n",
       "      <th>a_DD</th>\n",
       "      <th>...</th>\n",
       "      <th>shift_DD.2</th>\n",
       "      <th colspan=\"2\" halign=\"left\">shift_H.2</th>\n",
       "      <th colspan=\"2\" halign=\"left\">e_DD</th>\n",
       "      <th colspan=\"2\" halign=\"left\">e_H</th>\n",
       "      <th colspan=\"2\" halign=\"left\">6_DD</th>\n",
       "      <th colspan=\"2\" halign=\"left\">6_H</th>\n",
       "      <th colspan=\"2\" halign=\"left\">9_DD</th>\n",
       "      <th colspan=\"2\" halign=\"left\">9_H</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>...</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>49.5</td>\n",
       "      <td>29.011492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.32</td>\n",
       "      <td>11.259008</td>\n",
       "      <td>616.04</td>\n",
       "      <td>327.907844</td>\n",
       "      <td>110.37</td>\n",
       "      <td>11.723804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>262.70</td>\n",
       "      <td>76.729158</td>\n",
       "      <td>298.87</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>319.55</td>\n",
       "      <td>124.263599</td>\n",
       "      <td>381.92</td>\n",
       "      <td>264.897223</td>\n",
       "      <td>127.64</td>\n",
       "      <td>16.956582</td>\n",
       "      <td>1006.80</td>\n",
       "      <td>279.528174</td>\n",
       "      <td>117.76</td>\n",
       "      <td>14.999744</td>\n",
       "      <td>317.29</td>\n",
       "      <td>91.601497</td>\n",
       "      <td>97.32</td>\n",
       "      <td>14.726380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>49.5</td>\n",
       "      <td>29.011492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.54</td>\n",
       "      <td>11.584054</td>\n",
       "      <td>473.52</td>\n",
       "      <td>455.835264</td>\n",
       "      <td>103.80</td>\n",
       "      <td>12.369725</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>253.38</td>\n",
       "      <td>61.022728</td>\n",
       "      <td>390.32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>231.15</td>\n",
       "      <td>89.868633</td>\n",
       "      <td>266.45</td>\n",
       "      <td>64.900915</td>\n",
       "      <td>95.54</td>\n",
       "      <td>9.263178</td>\n",
       "      <td>503.36</td>\n",
       "      <td>233.809753</td>\n",
       "      <td>111.95</td>\n",
       "      <td>23.489252</td>\n",
       "      <td>108.04</td>\n",
       "      <td>58.899490</td>\n",
       "      <td>109.75</td>\n",
       "      <td>9.351038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>49.5</td>\n",
       "      <td>29.011492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>111.79</td>\n",
       "      <td>10.416862</td>\n",
       "      <td>280.73</td>\n",
       "      <td>56.253189</td>\n",
       "      <td>124.64</td>\n",
       "      <td>11.471986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153.89</td>\n",
       "      <td>25.397007</td>\n",
       "      <td>171.77</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>245.07</td>\n",
       "      <td>111.692874</td>\n",
       "      <td>205.23</td>\n",
       "      <td>33.898796</td>\n",
       "      <td>106.54</td>\n",
       "      <td>11.554369</td>\n",
       "      <td>298.53</td>\n",
       "      <td>189.088801</td>\n",
       "      <td>115.04</td>\n",
       "      <td>15.906240</td>\n",
       "      <td>102.93</td>\n",
       "      <td>74.708356</td>\n",
       "      <td>102.09</td>\n",
       "      <td>14.491443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>49.5</td>\n",
       "      <td>29.011492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67.54</td>\n",
       "      <td>15.381624</td>\n",
       "      <td>357.95</td>\n",
       "      <td>198.005019</td>\n",
       "      <td>53.97</td>\n",
       "      <td>7.137120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>274.30</td>\n",
       "      <td>129.159091</td>\n",
       "      <td>257.62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>734.85</td>\n",
       "      <td>547.656096</td>\n",
       "      <td>444.52</td>\n",
       "      <td>294.817508</td>\n",
       "      <td>62.50</td>\n",
       "      <td>12.402753</td>\n",
       "      <td>686.12</td>\n",
       "      <td>316.002087</td>\n",
       "      <td>49.28</td>\n",
       "      <td>10.886077</td>\n",
       "      <td>415.04</td>\n",
       "      <td>150.569687</td>\n",
       "      <td>49.63</td>\n",
       "      <td>7.816914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>49.5</td>\n",
       "      <td>29.011492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.37</td>\n",
       "      <td>16.682813</td>\n",
       "      <td>583.65</td>\n",
       "      <td>474.519116</td>\n",
       "      <td>79.11</td>\n",
       "      <td>10.074365</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>127.56</td>\n",
       "      <td>24.907230</td>\n",
       "      <td>291.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>129.58</td>\n",
       "      <td>27.665455</td>\n",
       "      <td>278.28</td>\n",
       "      <td>113.204390</td>\n",
       "      <td>70.12</td>\n",
       "      <td>9.224780</td>\n",
       "      <td>950.95</td>\n",
       "      <td>386.573248</td>\n",
       "      <td>85.71</td>\n",
       "      <td>16.465633</td>\n",
       "      <td>98.17</td>\n",
       "      <td>57.695483</td>\n",
       "      <td>66.58</td>\n",
       "      <td>11.096619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>49.5</td>\n",
       "      <td>29.011492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.38</td>\n",
       "      <td>13.208338</td>\n",
       "      <td>447.17</td>\n",
       "      <td>279.645557</td>\n",
       "      <td>110.39</td>\n",
       "      <td>12.041926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>288.77</td>\n",
       "      <td>204.763320</td>\n",
       "      <td>193.34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>565.28</td>\n",
       "      <td>205.750564</td>\n",
       "      <td>459.54</td>\n",
       "      <td>357.489010</td>\n",
       "      <td>107.58</td>\n",
       "      <td>22.544313</td>\n",
       "      <td>835.99</td>\n",
       "      <td>317.600998</td>\n",
       "      <td>99.65</td>\n",
       "      <td>17.056872</td>\n",
       "      <td>142.13</td>\n",
       "      <td>59.186856</td>\n",
       "      <td>92.73</td>\n",
       "      <td>14.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>49.5</td>\n",
       "      <td>29.011492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.74</td>\n",
       "      <td>11.420007</td>\n",
       "      <td>345.12</td>\n",
       "      <td>341.841900</td>\n",
       "      <td>68.35</td>\n",
       "      <td>11.024651</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>248.27</td>\n",
       "      <td>66.275954</td>\n",
       "      <td>234.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>471.01</td>\n",
       "      <td>214.666502</td>\n",
       "      <td>373.09</td>\n",
       "      <td>224.854301</td>\n",
       "      <td>78.05</td>\n",
       "      <td>14.559786</td>\n",
       "      <td>519.25</td>\n",
       "      <td>361.318751</td>\n",
       "      <td>110.35</td>\n",
       "      <td>18.255413</td>\n",
       "      <td>259.53</td>\n",
       "      <td>123.001064</td>\n",
       "      <td>87.42</td>\n",
       "      <td>18.173229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>49.5</td>\n",
       "      <td>29.011492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.69</td>\n",
       "      <td>11.220197</td>\n",
       "      <td>306.97</td>\n",
       "      <td>69.566400</td>\n",
       "      <td>55.31</td>\n",
       "      <td>9.266748</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>187.32</td>\n",
       "      <td>29.673945</td>\n",
       "      <td>215.51</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>508.10</td>\n",
       "      <td>118.986503</td>\n",
       "      <td>347.88</td>\n",
       "      <td>133.437329</td>\n",
       "      <td>56.70</td>\n",
       "      <td>8.627534</td>\n",
       "      <td>366.72</td>\n",
       "      <td>105.234906</td>\n",
       "      <td>55.63</td>\n",
       "      <td>10.284494</td>\n",
       "      <td>202.27</td>\n",
       "      <td>94.244669</td>\n",
       "      <td>53.60</td>\n",
       "      <td>12.940423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>49.5</td>\n",
       "      <td>29.011492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113.31</td>\n",
       "      <td>13.758153</td>\n",
       "      <td>353.81</td>\n",
       "      <td>96.384195</td>\n",
       "      <td>84.14</td>\n",
       "      <td>9.255105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>109.36</td>\n",
       "      <td>8.571176</td>\n",
       "      <td>321.37</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>128.50</td>\n",
       "      <td>19.427410</td>\n",
       "      <td>336.56</td>\n",
       "      <td>121.156158</td>\n",
       "      <td>110.55</td>\n",
       "      <td>11.002640</td>\n",
       "      <td>687.92</td>\n",
       "      <td>270.886859</td>\n",
       "      <td>107.58</td>\n",
       "      <td>15.416887</td>\n",
       "      <td>136.92</td>\n",
       "      <td>77.856081</td>\n",
       "      <td>84.96</td>\n",
       "      <td>12.404724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>49.5</td>\n",
       "      <td>29.011492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.34</td>\n",
       "      <td>11.887707</td>\n",
       "      <td>312.93</td>\n",
       "      <td>110.892728</td>\n",
       "      <td>69.08</td>\n",
       "      <td>10.453050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>177.17</td>\n",
       "      <td>77.039155</td>\n",
       "      <td>160.76</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>209.12</td>\n",
       "      <td>74.790114</td>\n",
       "      <td>214.94</td>\n",
       "      <td>68.418619</td>\n",
       "      <td>74.88</td>\n",
       "      <td>12.442417</td>\n",
       "      <td>348.46</td>\n",
       "      <td>116.881829</td>\n",
       "      <td>74.70</td>\n",
       "      <td>14.459627</td>\n",
       "      <td>111.00</td>\n",
       "      <td>55.595582</td>\n",
       "      <td>67.85</td>\n",
       "      <td>10.705776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>49.5</td>\n",
       "      <td>29.011492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.40</td>\n",
       "      <td>8.321810</td>\n",
       "      <td>312.87</td>\n",
       "      <td>68.174858</td>\n",
       "      <td>81.77</td>\n",
       "      <td>6.702713</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>146.05</td>\n",
       "      <td>37.652418</td>\n",
       "      <td>176.21</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>324.14</td>\n",
       "      <td>415.371685</td>\n",
       "      <td>400.94</td>\n",
       "      <td>356.044567</td>\n",
       "      <td>92.43</td>\n",
       "      <td>10.834154</td>\n",
       "      <td>1131.85</td>\n",
       "      <td>327.970754</td>\n",
       "      <td>85.12</td>\n",
       "      <td>9.915074</td>\n",
       "      <td>85.43</td>\n",
       "      <td>48.616258</td>\n",
       "      <td>72.26</td>\n",
       "      <td>9.149289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>49.5</td>\n",
       "      <td>29.011492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.79</td>\n",
       "      <td>8.630806</td>\n",
       "      <td>361.88</td>\n",
       "      <td>74.489546</td>\n",
       "      <td>100.82</td>\n",
       "      <td>14.766451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>312.72</td>\n",
       "      <td>1258.931189</td>\n",
       "      <td>184.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>191.28</td>\n",
       "      <td>55.809394</td>\n",
       "      <td>210.35</td>\n",
       "      <td>74.265205</td>\n",
       "      <td>71.46</td>\n",
       "      <td>17.716294</td>\n",
       "      <td>607.84</td>\n",
       "      <td>169.100814</td>\n",
       "      <td>97.11</td>\n",
       "      <td>367.559104</td>\n",
       "      <td>137.52</td>\n",
       "      <td>63.686968</td>\n",
       "      <td>61.38</td>\n",
       "      <td>13.039009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>49.5</td>\n",
       "      <td>29.011492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.70</td>\n",
       "      <td>15.456358</td>\n",
       "      <td>597.99</td>\n",
       "      <td>327.949845</td>\n",
       "      <td>106.86</td>\n",
       "      <td>16.077477</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>307.82</td>\n",
       "      <td>90.312407</td>\n",
       "      <td>299.60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>513.22</td>\n",
       "      <td>311.519365</td>\n",
       "      <td>478.69</td>\n",
       "      <td>222.339930</td>\n",
       "      <td>103.98</td>\n",
       "      <td>13.546016</td>\n",
       "      <td>877.55</td>\n",
       "      <td>243.548322</td>\n",
       "      <td>98.86</td>\n",
       "      <td>10.477508</td>\n",
       "      <td>161.72</td>\n",
       "      <td>43.386001</td>\n",
       "      <td>86.34</td>\n",
       "      <td>11.197240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>49.5</td>\n",
       "      <td>29.011492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.58</td>\n",
       "      <td>9.126695</td>\n",
       "      <td>144.53</td>\n",
       "      <td>55.590395</td>\n",
       "      <td>74.20</td>\n",
       "      <td>8.640988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>129.24</td>\n",
       "      <td>15.934918</td>\n",
       "      <td>110.57</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.97</td>\n",
       "      <td>13.535927</td>\n",
       "      <td>97.65</td>\n",
       "      <td>49.796226</td>\n",
       "      <td>108.31</td>\n",
       "      <td>14.055945</td>\n",
       "      <td>192.87</td>\n",
       "      <td>63.506884</td>\n",
       "      <td>106.33</td>\n",
       "      <td>12.315757</td>\n",
       "      <td>63.20</td>\n",
       "      <td>29.088857</td>\n",
       "      <td>82.77</td>\n",
       "      <td>10.051237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>49.5</td>\n",
       "      <td>29.011492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.80</td>\n",
       "      <td>10.935994</td>\n",
       "      <td>411.27</td>\n",
       "      <td>231.214442</td>\n",
       "      <td>114.78</td>\n",
       "      <td>13.199770</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>266.12</td>\n",
       "      <td>58.344203</td>\n",
       "      <td>266.37</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>308.22</td>\n",
       "      <td>116.119808</td>\n",
       "      <td>327.87</td>\n",
       "      <td>95.145466</td>\n",
       "      <td>104.30</td>\n",
       "      <td>8.406948</td>\n",
       "      <td>610.77</td>\n",
       "      <td>193.764402</td>\n",
       "      <td>108.36</td>\n",
       "      <td>10.550044</td>\n",
       "      <td>146.33</td>\n",
       "      <td>37.834316</td>\n",
       "      <td>91.09</td>\n",
       "      <td>11.271753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        attempt            f_DD          f_H               j_DD              \\\n",
       "           mean        std mean  std    mean        std    mean         std   \n",
       "subject                                                                       \n",
       "0          49.5  29.011492  0.0  0.0  102.32  11.259008  616.04  327.907844   \n",
       "1          49.5  29.011492  0.0  0.0   96.54  11.584054  473.52  455.835264   \n",
       "2          49.5  29.011492  0.0  0.0  111.79  10.416862  280.73   56.253189   \n",
       "3          49.5  29.011492  0.0  0.0   67.54  15.381624  357.95  198.005019   \n",
       "4          49.5  29.011492  0.0  0.0   84.37  16.682813  583.65  474.519116   \n",
       "5          49.5  29.011492  0.0  0.0   94.38  13.208338  447.17  279.645557   \n",
       "6          49.5  29.011492  0.0  0.0   63.74  11.420007  345.12  341.841900   \n",
       "7          49.5  29.011492  0.0  0.0   59.69  11.220197  306.97   69.566400   \n",
       "8          49.5  29.011492  0.0  0.0  113.31  13.758153  353.81   96.384195   \n",
       "9          49.5  29.011492  0.0  0.0   57.34  11.887707  312.93  110.892728   \n",
       "10         49.5  29.011492  0.0  0.0   83.40   8.321810  312.87   68.174858   \n",
       "11         49.5  29.011492  0.0  0.0   62.79   8.630806  361.88   74.489546   \n",
       "12         49.5  29.011492  0.0  0.0   90.70  15.456358  597.99  327.949845   \n",
       "13         49.5  29.011492  0.0  0.0   88.58   9.126695  144.53   55.590395   \n",
       "14         49.5  29.011492  0.0  0.0   96.80  10.935994  411.27  231.214442   \n",
       "\n",
       "            j_H            shift_DD      shift_H                 a_DD  ...  \\\n",
       "           mean        std     mean  std    mean          std    mean  ...   \n",
       "subject                                                                ...   \n",
       "0        110.37  11.723804      0.0  0.0  262.70    76.729158  298.87  ...   \n",
       "1        103.80  12.369725      0.0  0.0  253.38    61.022728  390.32  ...   \n",
       "2        124.64  11.471986      0.0  0.0  153.89    25.397007  171.77  ...   \n",
       "3         53.97   7.137120      0.0  0.0  274.30   129.159091  257.62  ...   \n",
       "4         79.11  10.074365      0.0  0.0  127.56    24.907230  291.00  ...   \n",
       "5        110.39  12.041926      0.0  0.0  288.77   204.763320  193.34  ...   \n",
       "6         68.35  11.024651      0.0  0.0  248.27    66.275954  234.30  ...   \n",
       "7         55.31   9.266748      0.0  0.0  187.32    29.673945  215.51  ...   \n",
       "8         84.14   9.255105      0.0  0.0  109.36     8.571176  321.37  ...   \n",
       "9         69.08  10.453050      0.0  0.0  177.17    77.039155  160.76  ...   \n",
       "10        81.77   6.702713      0.0  0.0  146.05    37.652418  176.21  ...   \n",
       "11       100.82  14.766451      0.0  0.0  312.72  1258.931189  184.50  ...   \n",
       "12       106.86  16.077477      0.0  0.0  307.82    90.312407  299.60  ...   \n",
       "13        74.20   8.640988      0.0  0.0  129.24    15.934918  110.57  ...   \n",
       "14       114.78  13.199770      0.0  0.0  266.12    58.344203  266.37  ...   \n",
       "\n",
       "        shift_DD.2 shift_H.2                e_DD                 e_H  \\\n",
       "               std      mean         std    mean         std    mean   \n",
       "subject                                                                \n",
       "0              0.0    319.55  124.263599  381.92  264.897223  127.64   \n",
       "1              0.0    231.15   89.868633  266.45   64.900915   95.54   \n",
       "2              0.0    245.07  111.692874  205.23   33.898796  106.54   \n",
       "3              0.0    734.85  547.656096  444.52  294.817508   62.50   \n",
       "4              0.0    129.58   27.665455  278.28  113.204390   70.12   \n",
       "5              0.0    565.28  205.750564  459.54  357.489010  107.58   \n",
       "6              0.0    471.01  214.666502  373.09  224.854301   78.05   \n",
       "7              0.0    508.10  118.986503  347.88  133.437329   56.70   \n",
       "8              0.0    128.50   19.427410  336.56  121.156158  110.55   \n",
       "9              0.0    209.12   74.790114  214.94   68.418619   74.88   \n",
       "10             0.0    324.14  415.371685  400.94  356.044567   92.43   \n",
       "11             0.0    191.28   55.809394  210.35   74.265205   71.46   \n",
       "12             0.0    513.22  311.519365  478.69  222.339930  103.98   \n",
       "13             0.0    120.97   13.535927   97.65   49.796226  108.31   \n",
       "14             0.0    308.22  116.119808  327.87   95.145466  104.30   \n",
       "\n",
       "                       6_DD                 6_H                9_DD  \\\n",
       "               std     mean         std    mean         std    mean   \n",
       "subject                                                               \n",
       "0        16.956582  1006.80  279.528174  117.76   14.999744  317.29   \n",
       "1         9.263178   503.36  233.809753  111.95   23.489252  108.04   \n",
       "2        11.554369   298.53  189.088801  115.04   15.906240  102.93   \n",
       "3        12.402753   686.12  316.002087   49.28   10.886077  415.04   \n",
       "4         9.224780   950.95  386.573248   85.71   16.465633   98.17   \n",
       "5        22.544313   835.99  317.600998   99.65   17.056872  142.13   \n",
       "6        14.559786   519.25  361.318751  110.35   18.255413  259.53   \n",
       "7         8.627534   366.72  105.234906   55.63   10.284494  202.27   \n",
       "8        11.002640   687.92  270.886859  107.58   15.416887  136.92   \n",
       "9        12.442417   348.46  116.881829   74.70   14.459627  111.00   \n",
       "10       10.834154  1131.85  327.970754   85.12    9.915074   85.43   \n",
       "11       17.716294   607.84  169.100814   97.11  367.559104  137.52   \n",
       "12       13.546016   877.55  243.548322   98.86   10.477508  161.72   \n",
       "13       14.055945   192.87   63.506884  106.33   12.315757   63.20   \n",
       "14        8.406948   610.77  193.764402  108.36   10.550044  146.33   \n",
       "\n",
       "                        9_H             \n",
       "                std    mean        std  \n",
       "subject                                 \n",
       "0         91.601497   97.32  14.726380  \n",
       "1         58.899490  109.75   9.351038  \n",
       "2         74.708356  102.09  14.491443  \n",
       "3        150.569687   49.63   7.816914  \n",
       "4         57.695483   66.58  11.096619  \n",
       "5         59.186856   92.73  14.366667  \n",
       "6        123.001064   87.42  18.173229  \n",
       "7         94.244669   53.60  12.940423  \n",
       "8         77.856081   84.96  12.404724  \n",
       "9         55.595582   67.85  10.705776  \n",
       "10        48.616258   72.26   9.149289  \n",
       "11        63.686968   61.38  13.039009  \n",
       "12        43.386001   86.34  11.197240  \n",
       "13        29.088857   82.77  10.051237  \n",
       "14        37.834316   91.09  11.271753  \n",
       "\n",
       "[15 rows x 50 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean and standard deviation for columns\n",
    "stats = df.groupby(by=[\"subject\"]).describe()\n",
    "stats.iloc[:, (stats.columns.get_level_values(1)=='mean') | (stats.columns.get_level_values(1)=='std')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subject        True\n",
       "attempt        True\n",
       "f_DD           True\n",
       "f_H           False\n",
       "j_DD          False\n",
       "j_H           False\n",
       "shift_DD       True\n",
       "shift_H       False\n",
       "a_DD          False\n",
       "a_H           False\n",
       "r_DD          False\n",
       "r_H           False\n",
       "@_DD          False\n",
       "@_H           False\n",
       "shift_DD.1     True\n",
       "shift_H.1     False\n",
       "s_DD          False\n",
       "s_H           False\n",
       "shift_DD.2     True\n",
       "shift_H.2     False\n",
       "e_DD          False\n",
       "e_H           False\n",
       "6_DD          False\n",
       "6_H           False\n",
       "9_DD          False\n",
       "9_H           False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if columns has 0 \n",
    "(df==0).sum().apply(lambda x: bool(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from **Subject**, **Attempt**, **f_DD** and **shift_DD.*i*** if any other column has 0 that means that something went wrong with the preprocessing into csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1500 entries, 0 to 1499\n",
      "Data columns (total 26 columns):\n",
      "subject       1500 non-null int64\n",
      "attempt       1500 non-null int64\n",
      "f_DD          1500 non-null int64\n",
      "f_H           1500 non-null int64\n",
      "j_DD          1500 non-null int64\n",
      "j_H           1500 non-null int64\n",
      "shift_DD      1500 non-null int64\n",
      "shift_H       1500 non-null int64\n",
      "a_DD          1500 non-null int64\n",
      "a_H           1500 non-null int64\n",
      "r_DD          1500 non-null int64\n",
      "r_H           1500 non-null int64\n",
      "@_DD          1500 non-null int64\n",
      "@_H           1500 non-null int64\n",
      "shift_DD.1    1143 non-null float64\n",
      "shift_H.1     1143 non-null float64\n",
      "s_DD          1500 non-null int64\n",
      "s_H           1500 non-null int64\n",
      "shift_DD.2    1500 non-null int64\n",
      "shift_H.2     1500 non-null int64\n",
      "e_DD          1500 non-null int64\n",
      "e_H           1500 non-null int64\n",
      "6_DD          1500 non-null int64\n",
      "6_H           1500 non-null int64\n",
      "9_DD          1500 non-null int64\n",
      "9_H           1500 non-null int64\n",
      "dtypes: float64(2), int64(24)\n",
      "memory usage: 304.8 KB\n"
     ]
    }
   ],
   "source": [
    "# Info (check that every column was read with correct type)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns description\n",
    "<img src=\"https://www.researchgate.net/profile/Saket-Maheshwary/publication/322952671/figure/fig1/AS:590982592626691@1517912638914/Illustration-of-keystroke-timing-features-given-as-inputs-to-Deep-Secure.png\" width=\"500\" alt=\"columns_names_explanation\"/>\n",
    "\n",
    "\n",
    "Source of image by Saket Maheshwary: [Deep Secure: A Fast and Simple Neural Network based approach for User Authentication and Identification via Keystroke Dynamics](https://www.researchgate.net/figure/Illustration-of-keystroke-timing-features-given-as-inputs-to-Deep-Secure_fig1_322952671)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Subject - Is a id of the subject, who provided current record\n",
    "* Attempt - Is a number of a attempt of corresponding user marked with id.\n",
    "* *Key_name*_DD - Is a press to press (down down) time of previous key to a next key, where previous key is prvoided as *Key_name*\n",
    "* *Key_name*_H - Is a hold time of a corresponding key name "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "We didn't used *UD_time*, because this time is implied by 2 other measured factors, to be exact $UD = DD - H$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaN Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subject       False\n",
       "attempt       False\n",
       "f_DD          False\n",
       "f_H           False\n",
       "j_DD          False\n",
       "j_H           False\n",
       "shift_DD      False\n",
       "shift_H       False\n",
       "a_DD          False\n",
       "a_H           False\n",
       "r_DD          False\n",
       "r_H           False\n",
       "@_DD          False\n",
       "@_H           False\n",
       "shift_DD.1     True\n",
       "shift_H.1      True\n",
       "s_DD          False\n",
       "s_H           False\n",
       "shift_DD.2    False\n",
       "shift_H.2     False\n",
       "e_DD          False\n",
       "e_H           False\n",
       "6_DD          False\n",
       "6_H           False\n",
       "9_DD          False\n",
       "9_H           False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if there are any NaN's\n",
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **NaN** values that occured are from software deviations i.e. not recognizing correctly shift key after **@**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>attempt</th>\n",
       "      <th>f_DD</th>\n",
       "      <th>f_H</th>\n",
       "      <th>j_DD</th>\n",
       "      <th>j_H</th>\n",
       "      <th>shift_DD</th>\n",
       "      <th>shift_H</th>\n",
       "      <th>a_DD</th>\n",
       "      <th>a_H</th>\n",
       "      <th>r_DD</th>\n",
       "      <th>r_H</th>\n",
       "      <th>@_DD</th>\n",
       "      <th>@_H</th>\n",
       "      <th>shift_DD.1</th>\n",
       "      <th>shift_H.1</th>\n",
       "      <th>s_DD</th>\n",
       "      <th>s_H</th>\n",
       "      <th>shift_DD.2</th>\n",
       "      <th>shift_H.2</th>\n",
       "      <th>e_DD</th>\n",
       "      <th>e_H</th>\n",
       "      <th>6_DD</th>\n",
       "      <th>6_H</th>\n",
       "      <th>9_DD</th>\n",
       "      <th>9_H</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>354</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>202</td>\n",
       "      <td>399</td>\n",
       "      <td>132</td>\n",
       "      <td>321</td>\n",
       "      <td>98</td>\n",
       "      <td>786</td>\n",
       "      <td>107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>276</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>728</td>\n",
       "      <td>455</td>\n",
       "      <td>95</td>\n",
       "      <td>575</td>\n",
       "      <td>127</td>\n",
       "      <td>79</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>2</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>279</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>166</td>\n",
       "      <td>179</td>\n",
       "      <td>90</td>\n",
       "      <td>136</td>\n",
       "      <td>331</td>\n",
       "      <td>113</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>960</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "      <td>1225</td>\n",
       "      <td>201</td>\n",
       "      <td>111</td>\n",
       "      <td>213</td>\n",
       "      <td>117</td>\n",
       "      <td>100</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>1212</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>1087</td>\n",
       "      <td>242</td>\n",
       "      <td>146</td>\n",
       "      <td>97</td>\n",
       "      <td>105</td>\n",
       "      <td>985</td>\n",
       "      <td>69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>598</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>1569</td>\n",
       "      <td>509</td>\n",
       "      <td>68</td>\n",
       "      <td>633</td>\n",
       "      <td>59</td>\n",
       "      <td>431</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>313</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>244</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>247</td>\n",
       "      <td>267</td>\n",
       "      <td>112</td>\n",
       "      <td>90</td>\n",
       "      <td>89</td>\n",
       "      <td>412</td>\n",
       "      <td>113</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>331</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>708</td>\n",
       "      <td>320</td>\n",
       "      <td>73</td>\n",
       "      <td>857</td>\n",
       "      <td>42</td>\n",
       "      <td>652</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>284</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>219</td>\n",
       "      <td>236</td>\n",
       "      <td>116</td>\n",
       "      <td>93</td>\n",
       "      <td>73</td>\n",
       "      <td>321</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>327</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>627</td>\n",
       "      <td>348</td>\n",
       "      <td>56</td>\n",
       "      <td>1118</td>\n",
       "      <td>36</td>\n",
       "      <td>442</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1267</td>\n",
       "      <td>12</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>495</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>288</td>\n",
       "      <td>256</td>\n",
       "      <td>104</td>\n",
       "      <td>296</td>\n",
       "      <td>96</td>\n",
       "      <td>559</td>\n",
       "      <td>96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>664</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>1376</td>\n",
       "      <td>695</td>\n",
       "      <td>96</td>\n",
       "      <td>695</td>\n",
       "      <td>95</td>\n",
       "      <td>168</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1268</td>\n",
       "      <td>12</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>471</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>336</td>\n",
       "      <td>296</td>\n",
       "      <td>112</td>\n",
       "      <td>471</td>\n",
       "      <td>96</td>\n",
       "      <td>503</td>\n",
       "      <td>104</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>383</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>783</td>\n",
       "      <td>400</td>\n",
       "      <td>103</td>\n",
       "      <td>695</td>\n",
       "      <td>112</td>\n",
       "      <td>215</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>12</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>480</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>256</td>\n",
       "      <td>239</td>\n",
       "      <td>143</td>\n",
       "      <td>375</td>\n",
       "      <td>88</td>\n",
       "      <td>464</td>\n",
       "      <td>120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>311</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>752</td>\n",
       "      <td>447</td>\n",
       "      <td>120</td>\n",
       "      <td>607</td>\n",
       "      <td>112</td>\n",
       "      <td>207</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1272</td>\n",
       "      <td>12</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>415</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>311</td>\n",
       "      <td>247</td>\n",
       "      <td>104</td>\n",
       "      <td>256</td>\n",
       "      <td>104</td>\n",
       "      <td>503</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>519</td>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>584</td>\n",
       "      <td>96</td>\n",
       "      <td>695</td>\n",
       "      <td>128</td>\n",
       "      <td>207</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1452</td>\n",
       "      <td>14</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>441</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>252</td>\n",
       "      <td>268</td>\n",
       "      <td>86</td>\n",
       "      <td>155</td>\n",
       "      <td>82</td>\n",
       "      <td>362</td>\n",
       "      <td>104</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>272</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>669</td>\n",
       "      <td>384</td>\n",
       "      <td>112</td>\n",
       "      <td>1897</td>\n",
       "      <td>100</td>\n",
       "      <td>102</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>357 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      subject  attempt  f_DD  f_H  j_DD  j_H  shift_DD  shift_H  a_DD  a_H  \\\n",
       "107         1        7     0   90   354  106         0      202   399  132   \n",
       "246         2       46     0  132   279  120         0      142   166  179   \n",
       "312         3       12     0   86  1212   58         0     1087   242  146   \n",
       "313         3       13     0   74   244   56         0      247   267  112   \n",
       "315         3       15     0   46   284   53         0      219   236  116   \n",
       "...       ...      ...   ...  ...   ...  ...       ...      ...   ...  ...   \n",
       "1267       12       67     0   95   495   96         0      288   256  104   \n",
       "1268       12       68     0   96   471  120         0      336   296  112   \n",
       "1270       12       70     0  112   480  120         0      256   239  143   \n",
       "1272       12       72     0   88   415  120         0      311   247  104   \n",
       "1452       14       52     0   95   441  120         0      252   268   86   \n",
       "\n",
       "      r_DD  r_H  @_DD  @_H  shift_DD.1  shift_H.1  s_DD  s_H  shift_DD.2  \\\n",
       "107    321   98   786  107         NaN        NaN   276  101           0   \n",
       "246     90  136   331  113         NaN        NaN   960  118           0   \n",
       "312     97  105   985   69         NaN        NaN   598   80           0   \n",
       "313     90   89   412  113         NaN        NaN   331  110           0   \n",
       "315     93   73   321  100         NaN        NaN   327   76           0   \n",
       "...    ...  ...   ...  ...         ...        ...   ...  ...         ...   \n",
       "1267   296   96   559   96         NaN        NaN   664  103           0   \n",
       "1268   471   96   503  104         NaN        NaN   383   80           0   \n",
       "1270   375   88   464  120         NaN        NaN   311  120           0   \n",
       "1272   256  104   503  128         NaN        NaN   519  104           0   \n",
       "1452   155   82   362  104         NaN        NaN   272  103           0   \n",
       "\n",
       "      shift_H.2  e_DD  e_H  6_DD  6_H  9_DD  9_H  \n",
       "107         728   455   95   575  127    79  111  \n",
       "246        1225   201  111   213  117   100   89  \n",
       "312        1569   509   68   633   59   431   34  \n",
       "313         708   320   73   857   42   652   58  \n",
       "315         627   348   56  1118   36   442   47  \n",
       "...         ...   ...  ...   ...  ...   ...  ...  \n",
       "1267       1376   695   96   695   95   168  103  \n",
       "1268        783   400  103   695  112   215   96  \n",
       "1270        752   447  120   607  112   207  104  \n",
       "1272       1000   584   96   695  128   207  104  \n",
       "1452        669   384  112  1897  100   102   99  \n",
       "\n",
       "[357 rows x 26 columns]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The NaN values\n",
    "df[df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fill the NaN values with average for corresponding users, (shift_DD fill with 0 as it should be)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['shift_DD.1'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby by subjects and get their mean of shift_H column\n",
    "means_shift = df.groupby(by=[\"subject\"])['shift_H.1'].mean()\n",
    "\n",
    "# For every user choose a rows of that user when there is NaN in column shift_H.1 and replace NaNs with adequate mean\n",
    "for user in df['subject'].unique():\n",
    "    df.loc[(df['subject']==user) & (df['shift_H.1'].isna()), 'shift_H.1'] = means_shift[user]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How algorithms will do in simple users classification\n",
    "We will use some AI algorithms to see results of training models with user data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# For ANN\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>f_H</th>\n",
       "      <th>j_DD</th>\n",
       "      <th>j_H</th>\n",
       "      <th>shift_H</th>\n",
       "      <th>a_DD</th>\n",
       "      <th>a_H</th>\n",
       "      <th>r_DD</th>\n",
       "      <th>r_H</th>\n",
       "      <th>@_DD</th>\n",
       "      <th>@_H</th>\n",
       "      <th>shift_H.1</th>\n",
       "      <th>s_DD</th>\n",
       "      <th>s_H</th>\n",
       "      <th>shift_H.2</th>\n",
       "      <th>e_DD</th>\n",
       "      <th>e_H</th>\n",
       "      <th>6_DD</th>\n",
       "      <th>6_H</th>\n",
       "      <th>9_DD</th>\n",
       "      <th>9_H</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>776</td>\n",
       "      <td>125</td>\n",
       "      <td>329</td>\n",
       "      <td>380</td>\n",
       "      <td>103</td>\n",
       "      <td>449</td>\n",
       "      <td>180</td>\n",
       "      <td>793</td>\n",
       "      <td>98</td>\n",
       "      <td>226.0</td>\n",
       "      <td>653</td>\n",
       "      <td>140</td>\n",
       "      <td>286</td>\n",
       "      <td>692</td>\n",
       "      <td>118</td>\n",
       "      <td>1454</td>\n",
       "      <td>110</td>\n",
       "      <td>268</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>442</td>\n",
       "      <td>112</td>\n",
       "      <td>269</td>\n",
       "      <td>271</td>\n",
       "      <td>118</td>\n",
       "      <td>326</td>\n",
       "      <td>137</td>\n",
       "      <td>983</td>\n",
       "      <td>160</td>\n",
       "      <td>319.0</td>\n",
       "      <td>708</td>\n",
       "      <td>125</td>\n",
       "      <td>416</td>\n",
       "      <td>398</td>\n",
       "      <td>125</td>\n",
       "      <td>1219</td>\n",
       "      <td>168</td>\n",
       "      <td>286</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>113</td>\n",
       "      <td>631</td>\n",
       "      <td>112</td>\n",
       "      <td>212</td>\n",
       "      <td>315</td>\n",
       "      <td>125</td>\n",
       "      <td>290</td>\n",
       "      <td>135</td>\n",
       "      <td>702</td>\n",
       "      <td>116</td>\n",
       "      <td>232.0</td>\n",
       "      <td>534</td>\n",
       "      <td>145</td>\n",
       "      <td>258</td>\n",
       "      <td>335</td>\n",
       "      <td>182</td>\n",
       "      <td>1153</td>\n",
       "      <td>124</td>\n",
       "      <td>310</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>543</td>\n",
       "      <td>120</td>\n",
       "      <td>312</td>\n",
       "      <td>341</td>\n",
       "      <td>110</td>\n",
       "      <td>321</td>\n",
       "      <td>97</td>\n",
       "      <td>662</td>\n",
       "      <td>91</td>\n",
       "      <td>412.0</td>\n",
       "      <td>688</td>\n",
       "      <td>149</td>\n",
       "      <td>348</td>\n",
       "      <td>377</td>\n",
       "      <td>132</td>\n",
       "      <td>1372</td>\n",
       "      <td>103</td>\n",
       "      <td>329</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>909</td>\n",
       "      <td>107</td>\n",
       "      <td>255</td>\n",
       "      <td>258</td>\n",
       "      <td>121</td>\n",
       "      <td>278</td>\n",
       "      <td>108</td>\n",
       "      <td>826</td>\n",
       "      <td>73</td>\n",
       "      <td>385.0</td>\n",
       "      <td>1044</td>\n",
       "      <td>117</td>\n",
       "      <td>385</td>\n",
       "      <td>475</td>\n",
       "      <td>120</td>\n",
       "      <td>1047</td>\n",
       "      <td>116</td>\n",
       "      <td>774</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1495</td>\n",
       "      <td>14</td>\n",
       "      <td>99</td>\n",
       "      <td>248</td>\n",
       "      <td>137</td>\n",
       "      <td>188</td>\n",
       "      <td>138</td>\n",
       "      <td>103</td>\n",
       "      <td>151</td>\n",
       "      <td>103</td>\n",
       "      <td>290</td>\n",
       "      <td>137</td>\n",
       "      <td>295.0</td>\n",
       "      <td>370</td>\n",
       "      <td>137</td>\n",
       "      <td>269</td>\n",
       "      <td>252</td>\n",
       "      <td>112</td>\n",
       "      <td>662</td>\n",
       "      <td>117</td>\n",
       "      <td>118</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1496</td>\n",
       "      <td>14</td>\n",
       "      <td>78</td>\n",
       "      <td>232</td>\n",
       "      <td>127</td>\n",
       "      <td>201</td>\n",
       "      <td>162</td>\n",
       "      <td>116</td>\n",
       "      <td>134</td>\n",
       "      <td>131</td>\n",
       "      <td>296</td>\n",
       "      <td>122</td>\n",
       "      <td>271.0</td>\n",
       "      <td>306</td>\n",
       "      <td>111</td>\n",
       "      <td>246</td>\n",
       "      <td>241</td>\n",
       "      <td>104</td>\n",
       "      <td>691</td>\n",
       "      <td>117</td>\n",
       "      <td>118</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1497</td>\n",
       "      <td>14</td>\n",
       "      <td>99</td>\n",
       "      <td>220</td>\n",
       "      <td>136</td>\n",
       "      <td>219</td>\n",
       "      <td>180</td>\n",
       "      <td>111</td>\n",
       "      <td>155</td>\n",
       "      <td>103</td>\n",
       "      <td>320</td>\n",
       "      <td>120</td>\n",
       "      <td>303.0</td>\n",
       "      <td>316</td>\n",
       "      <td>137</td>\n",
       "      <td>261</td>\n",
       "      <td>290</td>\n",
       "      <td>119</td>\n",
       "      <td>569</td>\n",
       "      <td>121</td>\n",
       "      <td>123</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1498</td>\n",
       "      <td>14</td>\n",
       "      <td>103</td>\n",
       "      <td>227</td>\n",
       "      <td>137</td>\n",
       "      <td>228</td>\n",
       "      <td>223</td>\n",
       "      <td>111</td>\n",
       "      <td>699</td>\n",
       "      <td>86</td>\n",
       "      <td>336</td>\n",
       "      <td>118</td>\n",
       "      <td>297.0</td>\n",
       "      <td>327</td>\n",
       "      <td>103</td>\n",
       "      <td>254</td>\n",
       "      <td>241</td>\n",
       "      <td>103</td>\n",
       "      <td>649</td>\n",
       "      <td>113</td>\n",
       "      <td>114</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1499</td>\n",
       "      <td>14</td>\n",
       "      <td>103</td>\n",
       "      <td>227</td>\n",
       "      <td>133</td>\n",
       "      <td>211</td>\n",
       "      <td>227</td>\n",
       "      <td>120</td>\n",
       "      <td>150</td>\n",
       "      <td>112</td>\n",
       "      <td>379</td>\n",
       "      <td>120</td>\n",
       "      <td>282.0</td>\n",
       "      <td>353</td>\n",
       "      <td>108</td>\n",
       "      <td>236</td>\n",
       "      <td>226</td>\n",
       "      <td>111</td>\n",
       "      <td>499</td>\n",
       "      <td>112</td>\n",
       "      <td>114</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      subject  f_H  j_DD  j_H  shift_H  a_DD  a_H  r_DD  r_H  @_DD  @_H  \\\n",
       "0           0  101   776  125      329   380  103   449  180   793   98   \n",
       "1           0  103   442  112      269   271  118   326  137   983  160   \n",
       "2           0  113   631  112      212   315  125   290  135   702  116   \n",
       "3           0   91   543  120      312   341  110   321   97   662   91   \n",
       "4           0  111   909  107      255   258  121   278  108   826   73   \n",
       "...       ...  ...   ...  ...      ...   ...  ...   ...  ...   ...  ...   \n",
       "1495       14   99   248  137      188   138  103   151  103   290  137   \n",
       "1496       14   78   232  127      201   162  116   134  131   296  122   \n",
       "1497       14   99   220  136      219   180  111   155  103   320  120   \n",
       "1498       14  103   227  137      228   223  111   699   86   336  118   \n",
       "1499       14  103   227  133      211   227  120   150  112   379  120   \n",
       "\n",
       "      shift_H.1  s_DD  s_H  shift_H.2  e_DD  e_H  6_DD  6_H  9_DD  9_H  \n",
       "0         226.0   653  140        286   692  118  1454  110   268  118  \n",
       "1         319.0   708  125        416   398  125  1219  168   286   94  \n",
       "2         232.0   534  145        258   335  182  1153  124   310   70  \n",
       "3         412.0   688  149        348   377  132  1372  103   329   86  \n",
       "4         385.0  1044  117        385   475  120  1047  116   774   94  \n",
       "...         ...   ...  ...        ...   ...  ...   ...  ...   ...  ...  \n",
       "1495      295.0   370  137        269   252  112   662  117   118   95  \n",
       "1496      271.0   306  111        246   241  104   691  117   118   95  \n",
       "1497      303.0   316  137        261   290  119   569  121   123   86  \n",
       "1498      297.0   327  103        254   241  103   649  113   114   78  \n",
       "1499      282.0   353  108        236   226  111   499  112   114   82  \n",
       "\n",
       "[1500 rows x 21 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We don't need the attempt column\n",
    "df.drop('attempt', axis=1, inplace=True)\n",
    "# We also don't need columns with all 0, because it only impacts dimensionality of our dataset\n",
    "df = df.loc[:, (df != 0).any(axis=0)]\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into attributes and labels\n",
    "X = df.drop('subject', axis=1)\n",
    "y = df['subject']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the metric we will use **accuracy** as we have exactly balanced classes,\n",
    "so accuracy should be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores of models for 4 fold stratified cross validation\n",
      "SVC score: 0.93\n",
      "KNN score: 0.92\n"
     ]
    }
   ],
   "source": [
    "# Let's do training with cross validation to get the performance of models \n",
    "\n",
    "# Lists to hold metric (accuracy) of models\n",
    "svm_scores = []\n",
    "knn_scores = []\n",
    "\n",
    "# 75% (train) / 25% (test)\n",
    "skf = StratifiedKFold(n_splits=4, random_state=101, shuffle=True)\n",
    "skf.get_n_splits(X, y)\n",
    "\n",
    "# We are cross validating with k=4 folds (stratified sampling) in for loop\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    # Get the train/test sets\n",
    "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Scaling data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # Fit models\n",
    "    svc_m = SVC().fit(X_train, y_train)\n",
    "    knn_m = KNeighborsClassifier(n_neighbors=5).fit(X_train, y_train)\n",
    "    \n",
    "    # Add metrics based on current fold's predictions\n",
    "    svm_scores.append(accuracy_score(y_test, svc_m.predict(X_test)))\n",
    "    knn_scores.append(accuracy_score(y_test, knn_m.predict(X_test)))\n",
    "\n",
    "print('Scores of models for 4 fold stratified cross validation')\n",
    "print(f'SVC score: {np.mean(svm_scores):.2f}')\n",
    "print(f'KNN score: {np.mean(knn_scores):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also try to test ANN\n",
    "ann = Sequential()\n",
    "# Adding to model one hidden layer with 20 neurons and setting input dimensionality to number of features\n",
    "ann.add(Dense(units=20, input_dim=20, activation='relu'))\n",
    "# Adding some dropouts in layer, arbitrary percentage\n",
    "ann.add(Dropout(0.2))\n",
    "# Output layer with neurons equal to classes (subjects)\n",
    "ann.add(Dense(units=len(y.unique()), activation='softmax'))\n",
    "\n",
    "# For a two or more label classes classification problem\n",
    "ann.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data (stratified sampling)\n",
    "# Also we have to one hot encode the classes\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.3, random_state=101, stratify=y)\n",
    "# Saving labels\n",
    "y_labels = y_test\n",
    "# Encoding y\n",
    "y_train = pd.get_dummies(y_train).values\n",
    "y_test = pd.get_dummies(y_test).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before training we have to scale data (we chose the MinMax scaler)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 2.6928 - accuracy: 0.1086 - val_loss: 2.6868 - val_accuracy: 0.1467\n",
      "Epoch 2/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 2.6667 - accuracy: 0.1238 - val_loss: 2.6643 - val_accuracy: 0.1822\n",
      "Epoch 3/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 2.6453 - accuracy: 0.1286 - val_loss: 2.6399 - val_accuracy: 0.2133\n",
      "Epoch 4/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 2.6177 - accuracy: 0.1638 - val_loss: 2.6062 - val_accuracy: 0.2289\n",
      "Epoch 5/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 2.5844 - accuracy: 0.1781 - val_loss: 2.5696 - val_accuracy: 0.2289\n",
      "Epoch 6/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 2.5381 - accuracy: 0.1895 - val_loss: 2.5304 - val_accuracy: 0.2200\n",
      "Epoch 7/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 2.5009 - accuracy: 0.2095 - val_loss: 2.4917 - val_accuracy: 0.2689\n",
      "Epoch 8/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 2.4631 - accuracy: 0.2552 - val_loss: 2.4526 - val_accuracy: 0.3533\n",
      "Epoch 9/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 2.4198 - accuracy: 0.2629 - val_loss: 2.4096 - val_accuracy: 0.3800\n",
      "Epoch 10/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 2.3791 - accuracy: 0.3200 - val_loss: 2.3671 - val_accuracy: 0.4822\n",
      "Epoch 11/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 2.3408 - accuracy: 0.3381 - val_loss: 2.3224 - val_accuracy: 0.5444\n",
      "Epoch 12/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 2.2856 - accuracy: 0.4048 - val_loss: 2.2735 - val_accuracy: 0.5978\n",
      "Epoch 13/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 2.2498 - accuracy: 0.4076 - val_loss: 2.2226 - val_accuracy: 0.6444\n",
      "Epoch 14/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 2.1876 - accuracy: 0.4514 - val_loss: 2.1691 - val_accuracy: 0.6622\n",
      "Epoch 15/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 2.1395 - accuracy: 0.4581 - val_loss: 2.1163 - val_accuracy: 0.6733\n",
      "Epoch 16/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 2.0867 - accuracy: 0.4905 - val_loss: 2.0621 - val_accuracy: 0.7133\n",
      "Epoch 17/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 2.0420 - accuracy: 0.5086 - val_loss: 2.0097 - val_accuracy: 0.7289\n",
      "Epoch 18/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 2.0062 - accuracy: 0.4924 - val_loss: 1.9596 - val_accuracy: 0.7333\n",
      "Epoch 19/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 1.9199 - accuracy: 0.5352 - val_loss: 1.9085 - val_accuracy: 0.7467\n",
      "Epoch 20/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 1.8936 - accuracy: 0.5505 - val_loss: 1.8622 - val_accuracy: 0.7511\n",
      "Epoch 21/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.8584 - accuracy: 0.5343 - val_loss: 1.8176 - val_accuracy: 0.7622\n",
      "Epoch 22/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.7935 - accuracy: 0.5771 - val_loss: 1.7746 - val_accuracy: 0.7667\n",
      "Epoch 23/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.7543 - accuracy: 0.5543 - val_loss: 1.7316 - val_accuracy: 0.7756\n",
      "Epoch 24/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 1.7189 - accuracy: 0.5790 - val_loss: 1.6936 - val_accuracy: 0.7822\n",
      "Epoch 25/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.6921 - accuracy: 0.5933 - val_loss: 1.6549 - val_accuracy: 0.7933\n",
      "Epoch 26/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.6512 - accuracy: 0.5829 - val_loss: 1.6218 - val_accuracy: 0.8067\n",
      "Epoch 27/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 1.6054 - accuracy: 0.6171 - val_loss: 1.5846 - val_accuracy: 0.8044\n",
      "Epoch 28/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 1.5788 - accuracy: 0.6143 - val_loss: 1.5516 - val_accuracy: 0.8111\n",
      "Epoch 29/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 1.5524 - accuracy: 0.6210 - val_loss: 1.5206 - val_accuracy: 0.8133\n",
      "Epoch 30/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 1.5104 - accuracy: 0.6371 - val_loss: 1.4898 - val_accuracy: 0.8133\n",
      "Epoch 31/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 1.4759 - accuracy: 0.6514 - val_loss: 1.4600 - val_accuracy: 0.8178\n",
      "Epoch 32/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.4792 - accuracy: 0.6324 - val_loss: 1.4372 - val_accuracy: 0.8156\n",
      "Epoch 33/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.4227 - accuracy: 0.6790 - val_loss: 1.4085 - val_accuracy: 0.8244\n",
      "Epoch 34/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 1.4131 - accuracy: 0.6533 - val_loss: 1.3830 - val_accuracy: 0.8178\n",
      "Epoch 35/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 1.3676 - accuracy: 0.6571 - val_loss: 1.3571 - val_accuracy: 0.8267\n",
      "Epoch 36/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.3567 - accuracy: 0.6848 - val_loss: 1.3335 - val_accuracy: 0.8333\n",
      "Epoch 37/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 1.3289 - accuracy: 0.6781 - val_loss: 1.3117 - val_accuracy: 0.8311\n",
      "Epoch 38/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 1.2917 - accuracy: 0.6895 - val_loss: 1.2902 - val_accuracy: 0.8244\n",
      "Epoch 39/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.2745 - accuracy: 0.7000 - val_loss: 1.2683 - val_accuracy: 0.8378\n",
      "Epoch 40/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.2753 - accuracy: 0.6905 - val_loss: 1.2492 - val_accuracy: 0.8333\n",
      "Epoch 41/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 1.2399 - accuracy: 0.7010 - val_loss: 1.2325 - val_accuracy: 0.8378\n",
      "Epoch 42/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 1.1962 - accuracy: 0.7276 - val_loss: 1.2132 - val_accuracy: 0.8533\n",
      "Epoch 43/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.2064 - accuracy: 0.6895 - val_loss: 1.1967 - val_accuracy: 0.8422\n",
      "Epoch 44/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 1.1751 - accuracy: 0.7190 - val_loss: 1.1792 - val_accuracy: 0.8467\n",
      "Epoch 45/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 1.1776 - accuracy: 0.7114 - val_loss: 1.1655 - val_accuracy: 0.8511\n",
      "Epoch 46/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 1.1434 - accuracy: 0.7248 - val_loss: 1.1489 - val_accuracy: 0.8533\n",
      "Epoch 47/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 1.1429 - accuracy: 0.7152 - val_loss: 1.1356 - val_accuracy: 0.8578\n",
      "Epoch 48/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 1.1301 - accuracy: 0.7124 - val_loss: 1.1245 - val_accuracy: 0.8511\n",
      "Epoch 49/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.1033 - accuracy: 0.7238 - val_loss: 1.1111 - val_accuracy: 0.8622\n",
      "Epoch 50/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.0769 - accuracy: 0.7200 - val_loss: 1.0953 - val_accuracy: 0.8600\n",
      "Epoch 51/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.0836 - accuracy: 0.7267 - val_loss: 1.0859 - val_accuracy: 0.8600\n",
      "Epoch 52/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.0672 - accuracy: 0.7267 - val_loss: 1.0743 - val_accuracy: 0.8578\n",
      "Epoch 53/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.0530 - accuracy: 0.7448 - val_loss: 1.0639 - val_accuracy: 0.8622\n",
      "Epoch 54/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.0197 - accuracy: 0.7448 - val_loss: 1.0519 - val_accuracy: 0.8644\n",
      "Epoch 55/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 1.0295 - accuracy: 0.7200 - val_loss: 1.0401 - val_accuracy: 0.8689\n",
      "Epoch 56/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 1.0066 - accuracy: 0.7467 - val_loss: 1.0305 - val_accuracy: 0.8689\n",
      "Epoch 57/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.9699 - accuracy: 0.7486 - val_loss: 1.0207 - val_accuracy: 0.8689\n",
      "Epoch 58/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.9665 - accuracy: 0.7657 - val_loss: 1.0127 - val_accuracy: 0.8711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.9647 - accuracy: 0.7552 - val_loss: 1.0040 - val_accuracy: 0.8733\n",
      "Epoch 60/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.9605 - accuracy: 0.7429 - val_loss: 0.9987 - val_accuracy: 0.8711\n",
      "Epoch 61/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.9387 - accuracy: 0.7610 - val_loss: 0.9891 - val_accuracy: 0.8711\n",
      "Epoch 62/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.9500 - accuracy: 0.7400 - val_loss: 0.9814 - val_accuracy: 0.8733\n",
      "Epoch 63/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.9142 - accuracy: 0.7610 - val_loss: 0.9733 - val_accuracy: 0.8756\n",
      "Epoch 64/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.9240 - accuracy: 0.7724 - val_loss: 0.9673 - val_accuracy: 0.8756\n",
      "Epoch 65/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.8893 - accuracy: 0.7771 - val_loss: 0.9615 - val_accuracy: 0.8733\n",
      "Epoch 66/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.9109 - accuracy: 0.7600 - val_loss: 0.9560 - val_accuracy: 0.8778\n",
      "Epoch 67/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.9144 - accuracy: 0.7533 - val_loss: 0.9517 - val_accuracy: 0.8733\n",
      "Epoch 68/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.9079 - accuracy: 0.7638 - val_loss: 0.9482 - val_accuracy: 0.8711\n",
      "Epoch 69/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.8870 - accuracy: 0.7714 - val_loss: 0.9415 - val_accuracy: 0.8711\n",
      "Epoch 70/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.8505 - accuracy: 0.7857 - val_loss: 0.9336 - val_accuracy: 0.8711\n",
      "Epoch 71/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.8668 - accuracy: 0.7629 - val_loss: 0.9299 - val_accuracy: 0.8800\n",
      "Epoch 72/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.8218 - accuracy: 0.7914 - val_loss: 0.9233 - val_accuracy: 0.8756\n",
      "Epoch 73/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.8471 - accuracy: 0.7819 - val_loss: 0.9234 - val_accuracy: 0.8756\n",
      "Epoch 74/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.8570 - accuracy: 0.7638 - val_loss: 0.9182 - val_accuracy: 0.8822\n",
      "Epoch 75/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.8485 - accuracy: 0.7838 - val_loss: 0.9134 - val_accuracy: 0.8822\n",
      "Epoch 76/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.8108 - accuracy: 0.7790 - val_loss: 0.9119 - val_accuracy: 0.8844\n",
      "Epoch 77/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.8519 - accuracy: 0.7743 - val_loss: 0.9086 - val_accuracy: 0.8822\n",
      "Epoch 78/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.7913 - accuracy: 0.7914 - val_loss: 0.9032 - val_accuracy: 0.8844\n",
      "Epoch 79/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.8066 - accuracy: 0.7943 - val_loss: 0.9006 - val_accuracy: 0.8889\n",
      "Epoch 80/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.7894 - accuracy: 0.7914 - val_loss: 0.9003 - val_accuracy: 0.8778\n",
      "Epoch 81/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.7916 - accuracy: 0.7943 - val_loss: 0.8954 - val_accuracy: 0.8822\n",
      "Epoch 82/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.8137 - accuracy: 0.7771 - val_loss: 0.8966 - val_accuracy: 0.8844\n",
      "Epoch 83/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.8215 - accuracy: 0.7648 - val_loss: 0.8947 - val_accuracy: 0.8867\n",
      "Epoch 84/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.7655 - accuracy: 0.7943 - val_loss: 0.8915 - val_accuracy: 0.8867\n",
      "Epoch 85/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.7539 - accuracy: 0.8057 - val_loss: 0.8855 - val_accuracy: 0.8822\n",
      "Epoch 86/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.7683 - accuracy: 0.7971 - val_loss: 0.8840 - val_accuracy: 0.8844\n",
      "Epoch 87/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.7508 - accuracy: 0.8067 - val_loss: 0.8801 - val_accuracy: 0.8889\n",
      "Epoch 88/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.7707 - accuracy: 0.7981 - val_loss: 0.8818 - val_accuracy: 0.8844\n",
      "Epoch 89/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.7717 - accuracy: 0.7924 - val_loss: 0.8801 - val_accuracy: 0.8867\n",
      "Epoch 90/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.7287 - accuracy: 0.8019 - val_loss: 0.8784 - val_accuracy: 0.8867\n",
      "Epoch 91/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.7395 - accuracy: 0.7971 - val_loss: 0.8742 - val_accuracy: 0.8889\n",
      "Epoch 92/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.7406 - accuracy: 0.7895 - val_loss: 0.8736 - val_accuracy: 0.8867\n",
      "Epoch 93/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.7501 - accuracy: 0.7905 - val_loss: 0.8749 - val_accuracy: 0.8889\n",
      "Epoch 94/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.7294 - accuracy: 0.7952 - val_loss: 0.8755 - val_accuracy: 0.8867\n",
      "Epoch 95/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6984 - accuracy: 0.8057 - val_loss: 0.8688 - val_accuracy: 0.8889\n",
      "Epoch 96/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6972 - accuracy: 0.8190 - val_loss: 0.8684 - val_accuracy: 0.8844\n",
      "Epoch 97/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.7172 - accuracy: 0.8143 - val_loss: 0.8686 - val_accuracy: 0.8867\n",
      "Epoch 98/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.7034 - accuracy: 0.8029 - val_loss: 0.8653 - val_accuracy: 0.8867\n",
      "Epoch 99/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.7115 - accuracy: 0.8124 - val_loss: 0.8628 - val_accuracy: 0.8867\n",
      "Epoch 100/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.7132 - accuracy: 0.7971 - val_loss: 0.8653 - val_accuracy: 0.8844\n",
      "Epoch 101/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6904 - accuracy: 0.8010 - val_loss: 0.8636 - val_accuracy: 0.8867\n",
      "Epoch 102/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.7021 - accuracy: 0.8057 - val_loss: 0.8617 - val_accuracy: 0.8867\n",
      "Epoch 103/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6933 - accuracy: 0.8086 - val_loss: 0.8623 - val_accuracy: 0.8911\n",
      "Epoch 104/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6595 - accuracy: 0.8105 - val_loss: 0.8623 - val_accuracy: 0.8867\n",
      "Epoch 105/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.7088 - accuracy: 0.7952 - val_loss: 0.8635 - val_accuracy: 0.8867\n",
      "Epoch 106/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6638 - accuracy: 0.8210 - val_loss: 0.8633 - val_accuracy: 0.8933\n",
      "Epoch 107/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6383 - accuracy: 0.8238 - val_loss: 0.8584 - val_accuracy: 0.8911\n",
      "Epoch 108/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.6736 - accuracy: 0.8029 - val_loss: 0.8613 - val_accuracy: 0.8956\n",
      "Epoch 109/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6476 - accuracy: 0.8152 - val_loss: 0.8590 - val_accuracy: 0.8889\n",
      "Epoch 110/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.6366 - accuracy: 0.8238 - val_loss: 0.8577 - val_accuracy: 0.8933\n",
      "Epoch 111/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.6713 - accuracy: 0.8057 - val_loss: 0.8583 - val_accuracy: 0.8867\n",
      "Epoch 112/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.6403 - accuracy: 0.8248 - val_loss: 0.8581 - val_accuracy: 0.8978\n",
      "Epoch 113/120\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.6606 - accuracy: 0.8152 - val_loss: 0.8611 - val_accuracy: 0.8978\n",
      "Epoch 114/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6385 - accuracy: 0.8152 - val_loss: 0.8567 - val_accuracy: 0.9000\n",
      "Epoch 115/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6619 - accuracy: 0.8029 - val_loss: 0.8601 - val_accuracy: 0.8933\n",
      "Epoch 116/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6377 - accuracy: 0.8181 - val_loss: 0.8597 - val_accuracy: 0.8978\n",
      "Epoch 117/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6229 - accuracy: 0.8333 - val_loss: 0.8625 - val_accuracy: 0.8911\n",
      "Epoch 118/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6333 - accuracy: 0.8200 - val_loss: 0.8618 - val_accuracy: 0.8978\n",
      "Epoch 119/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.5978 - accuracy: 0.8333 - val_loss: 0.8587 - val_accuracy: 0.8956\n",
      "Epoch 120/120\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.6194 - accuracy: 0.8238 - val_loss: 0.8591 - val_accuracy: 0.8956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24aa7c16508>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "ann.fit(x=X_train, \n",
    "          y=y_train, \n",
    "          epochs=120,\n",
    "          validation_data=(X_test, y_test), verbose=1\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x24aa8d5b5c8>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can see loss and accuracy of training set and validation set\n",
    "# After ~120 epchos the model gets overtrained\n",
    "model_loss = pd.DataFrame(ann.history.history)\n",
    "model_loss.plot(figsize=(8,6), grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82        30\n",
      "           1       0.79      0.90      0.84        30\n",
      "           2       0.94      1.00      0.97        30\n",
      "           3       0.97      0.97      0.97        30\n",
      "           4       0.97      0.97      0.97        30\n",
      "           5       0.86      0.83      0.85        30\n",
      "           6       0.96      0.77      0.85        30\n",
      "           7       0.93      0.93      0.93        30\n",
      "           8       0.90      0.90      0.90        30\n",
      "           9       0.97      0.93      0.95        30\n",
      "          10       0.90      0.93      0.92        30\n",
      "          11       0.85      0.93      0.89        30\n",
      "          12       0.77      0.80      0.79        30\n",
      "          13       0.97      1.00      0.98        30\n",
      "          14       0.88      0.73      0.80        30\n",
      "\n",
      "    accuracy                           0.90       450\n",
      "   macro avg       0.90      0.90      0.89       450\n",
      "weighted avg       0.90      0.90      0.89       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How ANN performed\n",
    "predictions = np.argmax(ann.predict(X_test), axis=-1)\n",
    "print(classification_report(y_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try another method of evaluation\n",
    "We will try to apply anomaly detection evaluation with **One Class SVM** and distances from **K Nearest Neighbors**.\n",
    "Our goal is to gather one of each of **every user's data** and fill it with some of the rows of the **other users**. After that we will provide whole **concatenated** data to algorithms and check how good they can distinguish true user's rows vs impostors' rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models for anomaly detection\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_rows(data, n):\n",
    "    np.random.shuffle(data)\n",
    "    return data[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters \n",
    "# Users' id's\n",
    "users = df.subject.unique()\n",
    "# How many rows pull from other users\n",
    "n_impostors = 3\n",
    "# Cutoff threshold\n",
    "knn_cutoff = 3\n",
    "# Scores\n",
    "svm_scores = []\n",
    "knn_scores = []\n",
    "\n",
    "\n",
    "# For every user\n",
    "for user in users:\n",
    "    # Get data of true user\n",
    "    X = df[df['subject']==user].drop('subject', axis=1).values\n",
    "    # List for storing rows of other users\n",
    "    impostors_test = []\n",
    "    # For every other user (impostor) retrive n timings\n",
    "    for impostor in users:\n",
    "        if impostor != user:\n",
    "            # Collect n rows of other user\n",
    "            impostor_rows = get_user_rows(df[df['subject']==impostor].drop('subject', axis=1).values, n_impostors)\n",
    "            # Add impostor's rows\n",
    "            for rows in impostor_rows:\n",
    "                impostors_test.append(rows)\n",
    "\n",
    "    # Create set containing true user's rows and impostors' rows\n",
    "    X_test = np.concatenate((X, np.array(impostors_test)))\n",
    "    \n",
    "    # Create labels 1 as true user and -1 as impostor\n",
    "    y = np.ones((X_test.shape[0]))\n",
    "    y[-len(impostors_test):] = -1\n",
    "\n",
    "    # Shuffle data before applying to algorithm\n",
    "    data_concat = np.concatenate((X_test, y[:, None]), axis=1)\n",
    "    np.random.shuffle(data_concat)\n",
    "    X_shuffled, y_shuffled = data_concat[:, :-1], data_concat[:, -1]\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    X_shuffled = scaler.fit_transform(X_shuffled)\n",
    "    \n",
    "    # Create One class SVM and check how well it detected anomalies\n",
    "    detector_svm = OneClassSVM(nu=0.3, kernel='rbf', gamma='auto')\n",
    "    detector_svm.fit(X_shuffled)\n",
    "    preds_svm = detector_svm.predict(X_shuffled)\n",
    "\n",
    "    score1 = np.count_nonzero(preds_svm == y_shuffled)\n",
    "    # Add to later average the score\n",
    "    svm_scores.append(score1/len(y_shuffled))\n",
    "    \n",
    "    # Get nearest neighbors and also check how well it performed\n",
    "    detector_knn = NearestNeighbors(n_neighbors = 3)\n",
    "    detector_knn.fit(X_shuffled)\n",
    "    distances, indexes = detector_knn.kneighbors(X_shuffled)\n",
    "    \n",
    "    # Based on commented plot we earlier chose a cutoff value for knn \n",
    "    # plt.plot(distances.mean(axis =1))\n",
    "    anomaly_idxs = np.where(distances.mean(axis = 1) > knn_cutoff)\n",
    "    # Prepare comparable labels => every other user than at anomaly_idxs' index was considered true (1)\n",
    "    preds_knn = np.ones(len(y_shuffled))\n",
    "    preds_knn[anomaly_idxs] = -1\n",
    "    \n",
    "    score2 = np.count_nonzero(preds_knn == y_shuffled)\n",
    "    # Add to later average the score\n",
    "    knn_scores.append(score2/len(y_shuffled))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged anomaly classification scores\n",
      "One class SVM classification accuracy: 77.65258215962442%\n",
      "KNN classification accuracy: 74.88262910798122%\n"
     ]
    }
   ],
   "source": [
    "print('Averaged anomaly classification scores')\n",
    "print(f'One class SVM classification accuracy: {100.0 * np.mean(svm_scores)}%')\n",
    "print(f'KNN classification accuracy: {100.0 * np.mean(knn_scores)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "When it comes to scores of the anomaly detection, they are quite lower than in our\n",
    "simple classification. The reason for that may be the fact that we didn't deal with outliers in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
